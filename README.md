# STAD

### Gkioxari G, Malik J. Finding action tubes. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.

我们致力于解决视频中的动作检测问题。受二维图像目标检测最新进展的推动，我们使用源自形状和运动线索的丰富特征层次结构来构建动作模型。我们以两种方式融合外观和运动。首先，从图像区域提案开始，选择那些运动显著、因此更有可能包含动作的区域。这显著减少了需要处理的区域数量，并提高了计算速度。其次，我们提取时空特征表示，以使用卷积神经网络构建强分类器。我们将预测关联起来，以产生时间一致的检测结果，我们称之为动作管。我们证明了我们的方法在动作检测任务中优于其他技术。

### Weinzaepfel P, Harchaoui Z, Schmid C. Learning to Track for Spatio-Temporal Action Localization. In Proc. IEEE Int. Conf. Comput.

我们提出了一种在真实视频中实现时空动作定位的有效方法。该方法首先在帧级别检测提议，并结合静态和运动 CNN 特征对其进行评分。然后，它使用逐检测跟踪的方法在整个视频中跟踪高分提议。我们的跟踪器同时依赖于实例级和类级检测器。使用时空运动直方图（轨迹级描述符）结合 CNN 特征对轨迹进行评分。最后，我们使用轨迹级滑动窗口方法执行动作的时间定位。我们展示了在 UCF-Sports、J-HMDB 和 UCF-101 动作定位数据集上进行时空定位的实验结果，其中我们的方法优于现有技术，mAP 分别高出 15%、7% 和 12%。


### Saha S, Singh G, Sapienza M, Torr PHS, Cuzzolin F. Deep Learning for Detecting Multiple Space-Time Action Tubes in Videos

在本研究中，我们提出了一种在时间上未修剪的视频中对多个并发动作进行时空定位（检测）和分类的方法。我们的框架由三个阶段组成。在第一阶段，采用外观和运动检测网络从彩色图像和光流中定位和评分动作。在第二阶段，将外观网络检测与运动检测分数相结合，并按其各自的空间重叠比例进行增强。在第三阶段，通过动态规划解决两个能量最大化问题，构建最有可能与单个动作实例相关联的检测框序列（称为动作管）。在第一阶段，通过使用检测框的类别特定分数及其空间重叠随时间链接它们来构建跨越整个视频的动作路径；在第二阶段，通过确保所有组成检测框的标签一致性来执行时间修剪。我们在极具挑战性的 UCF101、J-HMDB-21 和 LIRIS-HARL 数据集上展示了算法的性能，全面超越了 SOTA 的成果，并显著提升了测试时的检测速度。我们在动作检测性能上实现了巨大的飞跃，在 UCF-101 和 J-HMDB-21 数据集上，mAP（平均精度）与 SOTA 相比分别提升了 20% 和 11%


### Peng X, Schmid C. Multi-region Two-Stream R-CNN for Action Detection

我们提出了一种用于真实视频中动作检测的多区域双流 R-CNN 模型。我们从基于 Faster R-CNN 的帧级动作检测入手，做出了三项贡献：（1）我们证明了运动区域提议网络可以生成高质量提议，这些提议与外观区域提议网络的提议相辅相成；（2）我们证明了在多帧上堆叠光流可以显著提升帧级动作检测效果；（3）我们在 Faster R-CNN 模型中嵌入了多区域方案，增加了关于身体部位的补充信息。然后，我们将帧级检测与维特比算法相结合，并使用最大子阵列法在时间上定位动作。在 UCF-Sports、J-HMDB 和 UCF101 动作检测数据集上的实验结果表明，我们的方法在帧地图平均得分和视频地图平均得分上均显著优于当前最佳方法。

### Wang L, Qiao Y, Tang X, Gool LV. Actionness Estimation Using Hybrid Fully Convolutional Networks

动作性（Actionness）的引入是为了量化在特定位置包含通用动作实例的可能性。准确高效地估计动作性在视频分析中至关重要，并且可能有益于其他相关任务，例如动作识别和动作检测。本文提出了一种用于动作性估计的新型深度架构，称为混合全卷积网络 (H-FCN)，它由外观 FCN (A-FCN) 和运动 FCN (M-FCN) 组成。这两个 FCN 利用深度模型的强大能力，分别从静态外观和动态运动的角度估计动作性图。此外，H-FCN 的全卷积特性使其能够高效处理任意大小的视频。我们在 Stanford40、UCF Sports 和 JHMDB 等高难度数据集上进行了实验，以验证 H-FCN 在动作性估计方面的有效性，结果表明我们的方法取得了优于现有方法的性能。此外，我们将估计的动作性图应用于动作提案生成和动作检测。我们的动作性图显著提升了这些任务的当前最佳性能。

### Yang Z, Gao J, Nevatia R. Spatio-Temporal Action Detection with Cascade Proposal and Location Anticipation.

在本研究中，我们致力于解决在时间上未修剪的视频中进行时空动作检测的问题。这是一项重要且具有挑战性的任务，因为在时间和空间上找到准确的人体动作对于分析大规模视频数据至关重要。为了解决这个问题，我们提出了一个用于帧级动作检测的级联提议和位置预测 (CPLA) 模型。该模型有几个突出的特点：(1) 采用级联区域提议网络 (casRPN) 生成动作提议，与单区域提议网络 (RPN) 相比，其定位精度更高；(2) 通过位置预测网络 (LAN) 利用动作的时空一致性，因此帧级动作检测并非独立进行。然后，通过求解链接分数最大化问题将帧级检测链接起来，并在时间上修剪成时空动作管。我们在具有挑战性的 UCF101 和 LIRIS-HARL 数据集上证明了该模型的有效性，均达到了最佳性能。

### Li D, Qiu Z, Dai Q, Yao T, Mei T. Recurrent Tubelet Proposal and Recognition Networks for Action Detection.
视频是一种信息密集型媒体，变化复杂，因此检测视频中的动作是一项极具挑战性的任务。现有方法主要为每个单独的帧或固定长度的片段独立生成动作提议，而忽略了它们之间的时间上下文。由于动作本质上是一系列运动，因此这种时间上下文关系对于动作检测至关重要。这促使我们在确定当前帧中的动作区域时利用前一帧中的局部动作提议。具体而言，我们提出了一种名为循环小管提议与识别 (RTPR) 网络的新型深度架构，以结合时间上下文进行动作检测。所提出的 RTPR 由两个相关网络组成，即循环小管提议 (RTP) 网络和循环小管识别 (RTR) 网络。RTP 通过区域提议网络初始化起始帧的动作提议，然后以循环方式估计下一帧中提议的运动。不同帧的动作提议链接在一起形成小管提议。 RTR 利用多通道架构，在每个通道中，将一个 tubelet 候选框输入到 CNN 和 LSTM 中，以循环识别 tubelet 中的动作。我们在四个基准数据集上进行了广泛的实验，并展示了优于当前最佳方法的结果。更值得注意的是，我们在 UCF-Sports、J-HMDB、UCF-101 和 AVA 数据集上分别获得了 98.6%、81.3%、77.9% 和 22.3% 的 mAP，比最佳竞争对手分别提升了 2.9%、4.3%、0.7% 和 3.9%。

### Alwando EHP, Chen YT, Fang WH. CNN-based multiple path search for action tube detection in videos.
本文提出了一种基于双流卷积神经网络 (CNN) 的有效方法，用于检测视频中的多个时空动作管。首先，提出了一种新颖的视频定位细化 (VLR) 方案，利用相邻帧之间的时间一致性，迭代地校正可能不准确的边界框。然后，为了提供更可靠的检测分数，本文考虑了一种新的融合策略，该策略不仅结合了双流网络的外观和流信息，还结合了运动显著性，其中运动显著性用于处理微小的摄像机运动。此外，本文还开发了一种高效的多路径搜索 (MPS) 算法，用于在单次运行中同时识别多条路径。在 MPS 的前向消息传递中，每个节点根据前几个阶段确定的累积分数存储规定数量的连接信息。之后，调用反向路径追踪，通过完全重用前向传递中生成的信息，无需重复搜索过程，同时找到所有多条路径。因此，可以降低产生的复杂度。模拟结果表明，与 VLR 和新的融合方案相结合，所提出的 MPS 总体上可以在四个公共数据集上提供比最先进的作品更优异的性能。

### Wu CY, Feichtenhofer C, Fan H, He K, Krahenbuhl P, Girshick R. Long-term feature banks for detailed video understan.
为了理解世界，我们人类需要不断将现在与过去联系起来，并将事件置于特定背景中。在本文中，我们使现有的视频模型能够做到这一点。我们提出了一个长期特征库——从整个视频中提取的支持性信息——来增强最先进的视频模型，否则这些模型只能观看 2-5 秒的短片段。我们的实验表明，使用长期特征库增强 3D 卷积网络可以在三个具有挑战性的视频数据集（AVA、EPIC-Kitchens 和 Charades）上获得最先进的结果。代码可在线获取。

### Ji S, Xu W, Yang M, Yu K. 3D convolutional neural networks for human action recognition
我们研究监控视频中人体动作的自动识别。目前大多数方法都基于从原始输入计算出的复杂手工特征构建分类器。卷积神经网络 (CNN) 是一种可以直接作用于原始输入的深度模型。然而，这类模型目前仅限于处理二维输入。本文，我们开发了一种用于动作识别的新型 3D CNN 模型。该模型通过执行 3D 卷积从空间和时间维度提取特征，从而捕获编码在多个相邻帧中的运动信息。所开发的模型从输入帧生成多通道信息，最终的特征表示融合了所有通道的信息。为了进一步提升性能，我们建议使用高级特征对输出进行正则化，并结合各种不同模型的预测。我们将所开发的模型应用于机场监控视频的真实环境中的人体动作识别，与基准方法相比，它们取得了更优异的性能。

### Gu C, Sun C, Vijayanarasimhan S, Pantofaru C, Ross DA, Toderici G, Li Y, Ricco S, Sukthankar R, Schmid C, Malik J. AVA: A Video Dataset of Spatio Temporally Localized Atomic Visual Actions
本文介绍了一个时空局部化的原子视觉动作 (AVA) 视频数据集。AVA 数据集对 437 个 15 分钟视频片段中的 80 个原子视觉动作进行了密集注释，这些动作在空间和时间上具有局部性，最终生成 1.59 M 个动作标签，并且每个人经常出现多个标签。该数据集的主要特点包括：(1) 定义原子视觉动作，而非复合动作；(2) 精确的时空注释，每个人可能有多个注释；(3) 在 15 分钟的视频片段中对这些原子动作进行了详尽的注释；(4) 人物在连续片段之间具有时间关联；(5) 使用电影来收集各种动作表征。这与现有的时空动作识别数据集不同，后者通常为短视频片段中的复合动作提供稀疏的注释。AVA 的场景逼真且动作复杂，暴露了动作识别的内在难度。为了进行基准测试，我们提出了一种基于当前最佳方法的全新动作定位方法，并在 JHMDB 和 UCF101-24 类别上展现了更佳的性能。虽然在现有数据集上创下了新的最佳水平，但在 AVA 上的整体结果较低，仅为 15.8% 的 mAP，这凸显了开发新视频理解方法的必要性。

### Carreira J, Zisserman A. Quo vadis, action recognition? a new model and the kinetics dataset
当前动作分类数据集（UCF-101 和 HMDB-51）中视频的匮乏，使得识别优秀的视频架构变得困难，因为大多数方法在现有的小规模基准测试中获得了类似的性能。本文根据新的 Kinetics 人体动作视频数据集重新评估了最先进的架构。Kinetics 的数据量比现有数据集大两个数量级，包含 400 个人体动作类别，每个类别包含超过 400 个视频片段，并且数据均来自逼真且具有挑战性的 YouTube 视频。我们分析了当前架构在该数据集上执行动作分类任务的表现，以及在 Kinetics 上进行预训练后，在较小的基准测试数据集上的性能提升。我们还引入了一种新的双流膨胀 3D 卷积网络 (I3D)，它基于二维卷积网络膨胀：将超深度图像分类卷积网络的滤波器和池化核扩展为 3D，从而能够从视频中学习无缝的时空特征提取器，同时充分利用成功的 ImageNet 架构设计及其参数。我们展示了在 Kinetics 上进行预训练后，I3D 模型在动作分类方面显著提升，在 HMDB-51 上达到 80.2%，在 UCF-101 上达到 97.9%。

### Zolfaghari M, Oliveira GL, Sedaghat N, Brox T. Chained multi-stream networks exploiting pose, motion, and apearance for action classification and detection
一般的人体动作识别需要理解各种视觉线索。本文提出了一种网络架构，用于计算并整合动作识别中最重要的视觉线索：姿势、运动和原始图像。为了实现整合，我们引入了一个马尔可夫链模型，该模型逐一添加线索。最终方法高效且适用于动作分类以及时空动作定位。这两项贡献显著提升了各自基准的性能。整体方法在 HMDB51、J-HMDB 和 NTU RGB+ D 数据集上实现了最佳的动作分类性能。此外，它在 UCF101 和 J-HMDB 数据集上也取得了最佳的时空动作定位结果。

### Feichtenhofer C, Fan H, Malik J, He K. Slowfast networks for video recognition.
我们提出了用于视频识别的SlowFast网络。我们的模型包含：(i) 一个以低帧率运行的慢速路径，用于捕捉空间语义；以及 (ii) 一个以高帧率运行的快速路径，用于捕捉精细时间分辨率的运动。快速路径可以通过减少其通道容量来实现轻量化，同时又能学习到对视频识别有用的时间信息。我们的模型在视频动作分类和检测方面均取得了优异的性能，而SlowFast概念的贡献则显著提升了性能。我们在主流视频识别基准测试（包括Kinetics、Charades和AVA）上实现了最佳准确率。代码已发布于：https://github.com/facebookresearch/SlowFast。

### Feichtenhofer C. X3d: Expanding architectures for efficient video recognitions
本文介绍了 X3D，这是一系列高效的视频网络，它能够沿着多个网络轴（空间、时间、宽度和深度）逐步扩展微型二维图像分类架构。受机器学习中特征选择方法的启发，我们采用了一种简单的逐步网络扩展方法，每一步仅扩展一个轴，从而实现了良好的准确率与复杂度之间的平衡。为了将 X3D 扩展到特定的目标复杂度，我们先进行逐步前向扩展，然后再进行后向收缩。X3D 实现了最佳性能，同时所需的乘加运算次数和参数数量分别减少了 4.8 倍和 5.5 倍，却达到了与之前研究相似的准确率。我们最令人惊讶的发现是，具有高时空分辨率的网络能够表现良好，同时网络宽度和参数数量也非常轻量。我们在视频分类和检测基准测试中报告了其在前所未有的效率下具有竞争力的准确率。代码可从以下网址获取：https://github.com/facebookresearch/SlowFast。

### Liu Y, Yang F, Ginhac D. ACDnet: An Action Detection network for real-time edge computing based on flow-guided feature approximation and memory aggregation. Pattern Recognit. Lett., 2021, 145: 118–126
解读人类动作需要理解场景的时空背景。基于卷积神经网络 (CNN) 的先进动作检测器采用双流或三维 CNN 架构，并已取得显著成果。然而，由于推理时空信息的系统复杂性，这些方法通常以非实时、离线的方式运行。因此，它们的高计算成本与新兴的现实场景（例如服务机器人或公共监控）不相符，因为这些场景需要在资源有限的边缘设备上进行检测。本文提出了 ACDnet，这是一个紧凑的动作检测网络，面向实时边缘计算，兼顾效率和准确性。它智能地利用连续视频帧之间的时间相干性来近似其 CNN 特征，而不是简单地提取它们。它还集成了过去视频帧的记忆特征聚合，以增强当前检测的稳定性，并隐式地对随时间推移的长时间线索进行建模。在公开基准数据集 UCF-24 和 JHMDB-21 上进行的实验表明，ACDnet 与 SSD 检测器集成后，能够稳健地实现远超实时（75 FPS）的检测速度。同时，与其他使用更重配置的顶级方法相比，它保持了合理的准确率（70.92 和 49.53 帧 mAP）。代码可在 https://github.com/dginhac/ACDnet 获取。
爱思唯尔

### K ¨op¨ukl¨u O, Wei X, Rigoll G. You only watch once: A unified cnn architecture for real-time spatiotemporal action localization. arXiv preprint arXiv:1911.06644, 201
时空动作定位需要将两个信息源合并到设计的架构中：（1）来自先前帧的时间信息和（2）来自关键帧的空间信息。当前最先进的方法通常使用单独的网络提取这些信息，并使用额外的融合机制来获得检测结果。在这项工作中，我们提出了YOWO，一种用于视频流中实时时空动作定位的统一CNN架构。YOWO是一种具有两个分支的单阶段架构，可同时提取时间和空间信息，并在一次评估中直接从视频片段中预测边界框和动作概率。由于整个架构是统一的，因此可以进行端到端优化。YOWO架构速度很快，在16帧输入片段上提供每秒34帧的速度，在8帧输入片段上提供每秒62帧的速度，这是目前时空动作定位任务中最快的最先进的架构。值得注意的是，YOWO 在 J-HMDB-21 和 UCF101-24 数据集上的表现超越了此前的最优结果，分别提升了约 3% 和约 12%。此外，YOWO 是第一个也是唯一一个在 AVA 数据集上取得有竞争力结果的单阶段架构。我们将代码和预训练模型公开发布。

### Zhao J, Snoek CG. Dance with flow: Two-in-one stream action detection. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, 9935–994
本文的目标是检测动作的时空范围。基于RGB和光流的双流检测网络以较大的模型规模和繁重的计算量为代价，提供了最先进的精度。我们建议将RGB和光流嵌入到一个带有新层的二合一流网络中。运动条件层从光流图像中提取运动信息，运动调制层利用这些信息生成用于调制低级RGB特征的变换参数。该方法易于嵌入到现有的外观或双流动作检测网络中，并进行端到端训练。实验表明，利用运动条件调制RGB特征可以提高检测精度。我们的二合一流仅需最先进的双流方法一半的计算量和参数，即可在UCF101-24、UCFSports和J-HMDB上取得令人瞩目的结果。

### Singh G, Saha S, Sapienza M, Torr PHS, Cuzzolin F. Online Real-Time Multiple Spatiotemporal Action Localisation and Prediction. In Proc. IEEE Int. Conf. Comput. Vis., 2016, 3657–3666.
我们提出了一个用于实时多时空 (S/T) 动作定位和分类的深度学习框架。目前最先进的方法只能离线工作，速度太慢，无法在实际环境中使用。为了克服它们的局限性，我们引入了两项重大进展。首先，我们采用实时 SSD（单次多框检测器）CNN 对每个视频帧中可能包含目标动作的检测框进行回归和分类。其次，我们设计了一种新颖高效的在线算法，从 SSD 帧级检测中逐步构建和标记“动作管”。因此，我们的系统不仅能够实时执行 S/T 检测，还能以在线方式进行早期动作预测。在极具挑战性的 UCF101-24 和 J-HMDB-21 基准测试中，我们在 S/T 动作定位和早期动作预测方面都取得了新的 SOTA 成果，甚至与顶级离线竞争对手相比也毫不逊色。据我们所知，我们的系统是第一个能够对 UCF101-24 未修剪视频执行在线 S/T 动作定位的实时（高达 40fps）系统。


### Chen S, Sun P, Xie E, Ge C, Wu J, Ma L, Shen J, Luo P. Watch Only Once: An End-to-End Video Action Detection Framework. In Proc. IEEE Int. Conf. Comput. Vis., 2021, 8158–8167
我们提出了一种端到端的视频动作检测流程，名为“仅观看一次”（WOO）。目前的方法要么将视频动作检测任务解耦为参与者定位和动作分类的独立阶段，要么在一个阶段内训练两个独立的模型。相比之下，我们的方法在一个统一的网络中同时解决了参与者定位和动作分类问题。通过统一主干网络并消除许多手工编写的组件，整个流程得到了显著简化。WOO 采用统一的视频主干网络，同时提取用于参与者定位和动作分类的特征。此外，我们将时空动作嵌入引入到我们的框架中，并设计了一个时空融合模块，以获得更具判别性、信息更丰富的特征，从而进一步提升动作分类性能。在 AVA 和 JHMDB 数据集上进行的大量实验表明，WOO 达到了最佳性能，同时与现有方法相比仍降低了高达 16.7% 的 GFLOP。我们希望我们的工作能够激发人们对动作检测传统方法的重新思考，并为端到端动作检测奠定坚实的基础。代码已公开。

### Sui L, Zhang CL, Gu L, Han F. A Simple and Efficient Pipeline to Build an End-to-End Spatial-Temporal Action Detector. arXiv preprint arXiv:2206.03064, 2022
时空动作检测是视频理解的重要组成部分。当前的时空动作检测方法大多使用目标检测器获取人物候选，并将这些候选人物分类到不同的动作类别中。所谓的两阶段方法在实际应用中非常繁重且难以应用。一些现有方法构建了单阶段流程，但普通的单阶段流程性能下降较大，需要额外的分类模块才能达到相当的性能。本文探索了一种简单有效的流程来构建强大的单阶段时空动作检测器。该流程由两部分组成：一部分是简单的端到端时空动作检测器。所提出的端到端检测器与当前基于候选框的检测器相比，架构略有变化，并且没有添加额外的动作分类模块。另一部分是一种新颖的标记策略，利用稀疏注释数据中的未标记帧。我们将该模型命名为 SE-STAD。所提出的 SE-STAD 实现了约 2% 的 mAP 提升和约 80% 的 FLOP 减少。我们的代码将在https://github.com/4paradigm-CV/SE-STAD发布。

### Tian Z, Shen C, Chen H, He T. Fcos: Fully convolutional one-stage object detection. In Proc. IEEE Int. Conf. Comput. Vis., 2019, 9627–9636.
我们提出了一种完全卷积的单阶段目标检测器 (FCOS)，以逐像素预测的方式解决目标检测问题，类似于语义分割。几乎所有最先进的目标检测器，例如 RetinaNet、SSD、YOLOv3 和 Faster R-CNN，都依赖于预定义的锚框。相比之下，我们提出的检测器 FCOS 无需锚框，也无需提案。通过消除预定义的锚框集，FCOS 完全避免了与锚框相关的复杂计算，例如在训练期间计算重叠。更重要的是，我们还避免了所有与锚框相关的超参数，这些超参数通常对最终的检测性能非常敏感。仅使用后处理非最大抑制 (NMS)，基于 ResNeXt-64x4d-101 的 FCOS 在单模型和单尺度测试中实现了 44.7% 的 AP，超越了之前的单阶段检测器，并且具有更简单的优势。我们首次展示了一个更简单、更灵活的检测框架，并实现了更高的检测准确率。我们希望提出的 FCOS 框架能够成为许多其他实例级任务的简单而强大的替代方案。代码可从以下网址获取：https://tinyurl. com/FCOSv1

### Chen L, Tong Z, Song Y, Wu G, Wang L. Efficient Video Action Detection with Token Dropout and Context Refinement. arXiv preprint arXiv:2304.08451, 2023
带有大规模视频标记的流视频片段会阻碍视觉变换器 (ViT) 进行有效识别，尤其是在视频动作检测中，因为精确识别参与者需要足够的时空表征。在本研究中，我们提出了一种基于 vanilla ViT 的高效视频动作检测 (EVAD) 端到端框架。我们的 EVAD 包含两种专门的视频动作检测设计。首先，我们从关键帧中心的角度提出了一种时空标记丢弃方法。在一个视频片段中，我们保留其关键帧中的所有标记，保留与来自其他帧的参与者动作相关的标记，并丢弃该片段中剩余的标记。其次，我们利用剩余的标记来细化场景上下文，以便更好地识别参与者身份。我们的动作检测器中的兴趣区域 (RoI) 被扩展到时间域。捕获的时空参与者身份表示通过具有注意机制的解码器中的场景上下文进行细化。这两种设计使我们的 EVAD 在保持准确率的同时保持了高效，这已在三个基准数据集（即 AVA、UCF101-24 和 JHMDB）上得到验证。与 vanilla ViT 主干相比，我们的 EVAD 将整体 GFLOP 降低了 43%，并将实时推理速度提高了 40%，且性能丝毫未受影响。此外，即使在计算成本相似的情况下，我们的 EVAD 也能在更高分辨率的输入下将性能提升 1.1 mAP。代码可从 https://github.com/MCG-NJU/EVAD 获取。

### Sun C, Shrivastava A, Vondrick C, Murphy KP, Sukthankar R, Schmid C. Actor-Centric Relation Network. In Proc. Eur. Conf. Comput. Vis., 2018, 1–17
目前最先进的时空动作定位方法依赖于帧级检测，并使用 3D 卷积神经网络 (ConvNets) 建模时间上下文。在此，我们更进一步，对时空关系进行建模，以捕捉人类行为者、相关物体和场景元素之间的交互，这些交互对于区分相似的人类动作至关重要。我们的方法采用弱监督学习，并使用以行为者为中心的关系网络 (ACRN) 自动挖掘相关元素。ACRN 从行为者和全局场景特征中计算并累积成对的关系信息，并生成用于动作分类的关系特征。它以神经网络的形式实现，可以与现有的动作检测系统联合训练。我们证明了 ACRN 的表现优于其他捕捉关系信息的方法，并且所提出的框架在 JHMDB 和 AVA 上的性能提升了最佳水平。对学习到的关系特征进行可视化，证实了我们的方法能够关注每个动作的相关关系。

### Ulutan O, Rallapalli S, Srivatsa M, Torres C, Manjunath B. Actor conditioned attention maps for video action detection. In Proc. IEEE Winter Conf. Appl. Comput. Vis., 2020, 527–536.
在观察涉及多个参与者的复杂事件时，人类并不会单独评估每个参与者，而是根据上下文进行推断。周围的上下文为理解动作提供了至关重要的信息。为此，我们建议用注意力模块取代感兴趣区域 (RoI) 池化，该模块会根据检测到的参与者对每个时空区域的相关性进行排序，而不是进行裁剪。我们将其称为参与者条件注意力图 (ACAM)，它会放大/抑制从整个场景中提取的特征。由此产生的参与者条件特征使模型聚焦于与条件参与者相关的区域。对于参与者定位，我们利用预训练的物体检测器，其迁移效果更佳。所提出的模型高效，我们的动作检测流程实现了近乎实时的性能。在 AVA 2.1 和 JHMDB 上的实验结果证明了注意力图的有效性，AVA 的 mAP 提升了 7，JHMDB 的 mAP 提升了 4。

### Jiang J, Cao Y, Song L, Zhang S, Li Y, Xu Z, Wu Q, Gan C, Zhang C, Yu G. Human centric spatio-temporal action localization. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshop, 2018.
本文介绍了我们针对 ActivityNet AVA 挑战赛的时空动作定位解决方案。我们的系统由三部分组成：人体检测器、动作分类模块和参与者-目标关系网络。由于 AVA 主要包含以人为中心的动作类别，我们首先应用区域提议网络 (RPN) 来检测视频中的人体。然后，我们通过对人体区域进行 ROI 池化操作进行动作分类。为了捕捉人与物体的关系，我们进一步设计了一个参与者-目标关系网络，该网络通过对 ROI 及其周边区域进行非局部操作来实现。最终，我们在两条赛道的验证集上获得了 25.63% 和 25.75% 的平均精度 (mAP)，在测试集上获得了 21.075% 和 20.99% 的平均精度。

### Ning Z, Xie Q, Zhou W, Wang L, Li H. Person-Context Cross Attention for Spatio-Temporal Action Detection. Technical report, Technical report, Huawei Noah’s Ark Lab, and University of Science and Technology of China, 
本技术报告介绍了我们在 ICCV 2021 DeeperAction 挑战赛中针对 Multi-Sports 时空动作检测的解决方案。我们的解决方案利用交叉注意力机制，明确地建模人物与场景之间的关系，以进行动作检测。我们描述了针对全新 MultiSports 数据集的解决方案细节以及一些实验结果。最终，我们在 MultiSports 测试集上实现了 48.68 的帧 mAP 和 24.2 的视频 mAP@0.1:0.9，并获得了 MultiSports 赛道的第一名，遥遥领先于其他参赛作品。

### Wang X, Girshick R, Gupta A, He K. Non-local neural networks. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, 7794–780
卷积运算和循环运算都是构建块，每次处理一个局部邻域。在本文中，我们将非局部运算作为一类通用的构建块，用于捕获长距离依赖关系。受计算机视觉中经典非局部均值方法的启发，我们的非局部运算将某个位置的响应计算为所有位置特征的加权和。该构建块可以嵌入到许多计算机视觉架构中。在视频分类任务中，即使没有任何额外的功能，我们的非局部模型也能在 Kinetics 和 Charades 数据集上与当前的竞赛获胜者竞争，甚至超越它们。在静态图像识别中，我们的非局部模型改进了 COCO 任务集上的物体检测/分割和姿态估计。代码即将发布。

### Wu J, Kuang Z, Wang L, Zhang W, Wu G. Context-aware rcnn: A baseline for action detection in videos. In Proc. Eur. Conf. Comput. Vis., 2020, 440–456
视频动作检测方法通常基于 Faster-RCNN 的标准流程，基于 RoI 池化特征进行以参与者为中心的动作识别。本研究首先通过实证研究发现，识别准确率与参与者的边界框大小高度相关，因此更高的参与者分辨率有助于提升性能。然而，视频模型需要实时密集采样才能实现精准识别。为了适应 GPU 内存，传输到主干网络的帧必须保持低分辨率，导致 RoI 池化层的特征图较为粗糙。因此，我们重新审视 RCNN 进行以参与者为中心的动作识别，方法是先裁剪并调整参与者周围的图像块大小，然后再使用 I3D 深度网络进行特征提取。此外，我们发现略微扩展参与者的边界框并融合上下文特征可以进一步提升性能。最终，我们开发了一个出乎意料的有效基线（上下文感知 RCNN），并在 AVA 和 JHMDB 两个具有挑战性的动作检测基准测试中取得了新的最佳结果。我们的观察结果挑战了基于 RoI-Pooling 流程的传统观点，并促使研究人员重新思考分辨率在以参与者为中心的动作识别中的重要性。我们的方法可以作为视频动作检测的强大基准，并有望为该领域带来新的思路。代码可在 https://github.com/MCG-NJU/CRCNN-Action 获取。
施普林格

### Girdhar R, Carreira J, Doersch C, Zisserman A. Video Action Transformer Network. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, 244–25
我们引入了 Action Transformer 模型，用于识别和定位视频片段中的人体动作。我们重新设计了 Transformer 风格的架构，以聚合我们试图对其动作进行分类的人物周围时空上下文的特征。我们展示了通过使用高分辨率、特定于人物且与类别无关的查询，该模型能够自发学习追踪人物并从他人动作中获取语义上下文。此外，它的注意力机制能够学习强调手部和面部特征，这些特征通常对于区分动作至关重要——所有这些都无需除边界框和类别标签之外的明确监督。我们在原子视觉动作 (AVA) 数据集上训练和测试了 Action Transformer 网络，仅使用原始 RGB 帧作为输入，其性能就显著超越了最先进的技术。

### Tang J, Xia J, Mu X, Pang B, Lu C. Asynchronous Interaction Aggregation for Action Detection. In Proc. Eur. Conf. Comput. Vis., 2020, 22–39
理解交互是视频动作检测的关键环节。我们提出了异步交互聚合网络 (AIA)，利用不同的交互来提升动作检测性能。它包含两个关键设计：一是交互聚合结构 (IA)，采用统一范式对多种类型的交互进行建模和集成；二是异步内存更新算法 (AMU)，该算法能够动态建模长期交互，而无需耗费大量的计算资源，从而实现更佳性能。我们提供的实证证据表明，我们的网络能够从集成交互中获得显著的准确率，并且易于端到端训练。我们的方法在 AVA 数据集上取得了最佳性能，与我们的强大基线相比，在验证集上mAP提升了 3.7（相对提升12.6%）。在 UCF101-24 和 EPIC-Kitchens 数据集上的结果进一步证明了我们方法的有效性。源代码将公开在：https://github.com/MVIG-SJTU/AlphAction。

### Zheng YD, Chen G, Yuan M, Lu T. MRSN: Multi-Relation Support Network for Video Action Detection. arXiv preprint arXiv:2304.11975, 2023
动作检测是一项极具挑战性的视频理解任务，需要对时空关系和交互关系进行建模。当前的方法通常分别对参与者-参与者关系和参与者-上下文关系进行建模，忽略了它们之间的互补性和相互支持性。为了解决这个问题，我们提出了一种名为多关系支持网络 (MRSN) 的新型网络。在 MRSN 中，参与者-上下文关系编码器 (ACRE) 和参与者-参与者关系编码器 (AARE) 分别对参与者-上下文关系和参与者-参与者关系进行建模。然后，关系支持编码器 (RSE) 计算两种关系之间的支持度并执行关系级别的交互。最后，关系共识模块 (RCM) 使用长期关系库 (LRB) 中的长期关系来增强这两种关系并达成共识。我们的实验表明，分别对关系进行建模并执行关系级别的交互可以在两个具有挑战性的视频数据集 AVA 和 UCF101-24 上取得甚至超越当前最佳结果。

### Faure GJ, Chen MH, Lai SH. Holistic Interaction Transformer Network for Action Detection. In Proc. IEEE Winter Conf. Appl. Comput. Vis., 2023.  
动作关乎我们如何与环境互动，包括与他人、物体以及自身互动。本文提出了一种新颖的多模态整体交互转换网络 (HIT)，它利用了那些通常被忽视但对大多数人类动作至关重要的手部和姿势信息。该 HIT 网络是一个全面的双模态框架，包含 RGB 流和姿势流。每个子网络分别对人、物体和手部交互进行建模。在每个子网络中，引入一个模态内聚合模块 (IMA)，用于选择性地合并各个交互单元。然后，使用注意力融合机制 (AFM) 将每个模态生成的特征粘合在一起。最后，我们从时间上下文中提取线索，以便利用缓存更好地对正在发生的动作进行分类。我们的方法在 J-HMDB、UCF101-24 和 MultiSports 数据集上的表现显著优于先前的方法。我们在 AVA 数据集上也取得了具有竞争力的结果。代码可在 https://github.com/joslefaure/HIT 获取。

### Pramono RRA, Chen YT, Fang WH. Hierarchical self- attention network for action localization in videos. In Proc. IEEE Int. Conf. Comput. Vis., 2019, 61–70
本文提出了一种新颖的分层自注意力网络 (HISAN)，用于生成视频中动作定位的时空管道。HISAN 的精髓在于将双流卷积神经网络 (CNN) 与分层双向自注意力机制相结合，该机制包含两个级别的双向自注意力机制，能够有效捕捉长期时间依赖信息和空间上下文信息，从而实现更精准的动作定位。此外，本文还采用了序列重评分 (SR) 算法来解决遮挡或背景杂乱导致的检测得分不一致的问题。此外，本文还提出了一种新的融合方案，不仅融合了双流网络的外观和运动信息，还融合了运动显著性信息，以减轻摄像机运动的影响。仿真结果表明，在广泛的 UCF101-24 和 J-HMDB 数据集上，新方法在动作定位和识别准确率方面达到了与当前最佳方法相当的水平。

### Pan J, Chen S, Shou Z, Shao J, Li H. Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, 464–474
从视频中定位人物并识别其动作是高阶视频理解领域的一项挑战。近期进展主要源于对实体之间直接成对关系的建模。本文更进一步，不仅对实体之间的直接关系进行建模，还考虑了基于多个元素建立的间接高阶关系。我们提出明确地建模“参与者-上下文-参与者关系”，即两个参与者基于其与上下文的交互而建立的关系。为此，我们设计了一个“参与者-上下文-参与者关系网络”（ACAR-Net），它基于一个新颖的高阶关系推理算子和一个参与者-上下文特征库，从而能够对时空动作定位进行间接关系推理。在 AVA 和 UCF101-24 数据集上的实验展示了对参与者-上下文-参与者关系建模的优势，而注意力图的可视化进一步验证了我们的模型能够找到相关的高阶关系来支持动作检测。值得注意的是，我们的方法在 2020 年 ActivityNet 挑战赛的 AVA-Kinetics 动作定位任务中排名第一，显著领先于其他参赛作品（+6.71 mAP）。代码已在线提供。

### Tomei M, Baraldi L, Calderara S, Bronzin S, Cucchiara R. Video action detection by learning graph-based spatio-temporal interactions. Comput. Vis. Image Understand., 2021, 206: 103187–103197
理解视频片段中人物的动作是计算机视觉领域的一个悬而未决的问题，这个问题已经解决了二十多年（Bobick and Davis, 2001；Herath et al., 2017）。过去，这项任务是通过为特定动作设计手工特征来解决的（Laptev, 2005；Vezzani et al., 2009）。最近，视频动作检测任务（Sun et al., 2018；Ulutan et al., 2020；Yang et al., 2019）与能够提取细粒度和判别性时空特征的深度架构一起引入，以紧凑且易于管理的形式表示视频块。这促使最近人们努力设计用于视频特征提取的新型主干网络（ Feichtenhofer et al., 2019；Tran et al., 2018；Wu et al., 2019）。另一方面，更高层次的推理对于检测和理解人类行为是必要的。
有趣的是，受目标检测架构启发的视频动作检测网络的性能仍然远未令人满意。例如，如果不考虑上下文，仅凭观察周围的边界框很难识别一个人是否在注视他人。这可以部分解释为先前的研究缺乏适当的上下文理解，因为它们无法建模参与者与周围元素之间的关系（Ulutan 等人，2020 年）。此外，场景中物体和其他人物的存在及其行为都会影响对当前参与者的理解。
高级推理不仅在空间层面上是必要的，以模拟紧密实体之间的关系，而且在时间层面上也是必要的：大多数现有的主干网络可以处理小的时间变化，而无需模拟长期的时间关系。
基于这些前提，我们设计了一个用于视频动作检测的高级模块，该模块考虑场景中不同人之间的交互以及演员和物体之间的交互。此外，它还可以通过在学习和推理过程中连接连续的剪辑来考虑时间依赖性。相同的模块可以堆叠多次以形成多层结构（图1）。以这种方式，可以任意增加整体时间感受野以模拟长距离依赖关系。由于我们的方法在特征级别工作，因此它可以在不显著增加其计算要求的情况下扩展其时间感受野。我们的解决方案可以利用现有的主干网进行特征提取，并且可以在不对底层主干网进行端到端微调的情况下实现最先进的结果。
先前在行为分析领域的研究已经尝试利用基于图的表征（Wang 和 Gupta，2018 年；Zhang 等人，2019 年），对与上下文的关系进行建模（Girdhar 等人，2019 年；Ulutan 等人，2020 年），并利用长期时间关系（Wu 等人，2019 年）：我们的提案将所有这些见解融合到一个模块中，该模块独立于特征提取层并处理预先计算的表征。此外，我们的模型是第一个在图边缘上也采用基于学习的方法的模型。我们在原子视觉动作 (AVA) 数据集 ( Gu et al., 2018 ) 上测试了我们的模型，该数据集是识别人类动作并利用情境作用的极具挑战性的测试平台。此外，我们还在 J-HMDB-21 ( Jhuang et al., 2013 ) 和 UCF101-24 ( Soomro et al., 2012 ) 上进行了实验。我们证明了我们的方法能够提升三种不同视频主干网络的性能，并在 AVA 2.1 和 AVA 2.2 上达到了最佳效果。
我们提出了一种新颖的视频动作检测模块，该模块考虑了演员和物体之间的时空关系。
•
所提出的模块基于视频的时空图表示，该表示通过自注意力操作进行学习。此外，可以将所提出的模块的多个实例堆叠在一起，以获得更大的时间感受野。整个模型独立于特征提取阶段，无需端到端训练即可获得最佳结果。
•
在极具挑战性的 AVA 数据集 ( Gu et al., 2018 ) 上进行的大量实验验证了我们的方法及其组件，并展现出优于其他端到端模型的性能。尤其值得一提的是，我们的方案在 AVA 2.1 和 AVA 2.2 上分别达到了 29.8 和 31.8 的mAP，超越了所有先前的方法。

### Ni J, Qin J, Huang D. Identity-aware Graph Memory Network for Action Detection. In Proc. ACM Multimedia Conf., 2021, 1–9
动作检测在高级视频理解和媒体解读中发挥着重要作用。许多现有研究通过建模上下文，捕捉视频中传达的参与者、物体和场景之间的关系，从而实现这种时空定位。然而，这些研究通常将参与者一概而论，而没有考虑个体之间的一致性和差异性，这留下了很大的改进空间。本文通过一个图记忆网络，即身份感知图记忆网络 (IGMN)，在长期和短期上下文中明确地突出参与者的身份信息。具体而言，我们提出了分层图神经网络 (HGNN)，用于在同一身份内部以及不同身份之间进行全面的长期关系建模。对于短期上下文，我们开发了一个双重注意模块 (DAM) 来生成身份感知约束，以减少不同身份参与者干扰的影响。在具有挑战性的 AVA 数据集上进行的大量实验证明了我们方法的有效性，并在 AVA v2.1 和 v2.2 上取得了最佳结果。

### Jain M, Van Gemert J, J ´egou H, Bouthemy P, Snoek CG. Action localization with tubelets from motion. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014, 740–74
本文探讨动作定位问题，其目标是确定特定动作出现的时间和地点。我们引入了一种采样策略来生成 2D+t 边界框序列（称为 tubelets）。与最先进的替代方案相比，这大大减少了可能包含目标动作的假设数量。我们的方法受到图像定位领域中一项最新技术的启发。除了首次将这项技术应用于视频之外，我们还重新审视了该策略在从超体素获得的 2D+t 序列中的应用。我们的采样策略有利地利用了一个反映动作相关运动如何偏离背景运动的标准。我们通过在两个公共数据集（UCF Sports 和 MSR-II）上进行大量实验证明了我们方法的有效性。我们的方法在这两个数据集上的表现都显著优于最先进的方法，同时将动作的搜索范围限制在可能的边界框序列的一小部分。

### Hou R, Chen C, Shah M. Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos. In Proc. IEEE Int. Conf. Comput. Vis., 2017, 5823–583
深度学习已被证明能在图像分类和目标检测方面取得优异的成果。然而，由于视频数据的复杂性和标注的缺乏，深度学习在视频分析（例如动作检测和识别）方面的影响有限。以往基于卷积神经网络 (CNN) 的视频动作检测方法通常包含两个主要步骤：帧级动作候选框检测和跨帧候选框关联。此外，这些方法采用双流 CNN 框架分别处理空间和时间特征。本文提出了一种端到端的深度网络，称为管状卷积神经网络 (T-CNN)，用于视频动作检测。该架构是一个统一的网络，能够基于 3D 卷积特征识别和定位动作。首先将视频分成等长的片段，然后基于 3D 卷积网络 (ConvNet) 特征为每个片段生成一组管状候选框。最后，利用网络流将不同片段的管状候选框链接在一起，并使用这些链接的视频候选框执行时空动作检测。在多个视频数据集上进行的大量实验表明，与最先进的技术相比，T-CNN 在对修剪和未修剪视频中的动作进行分类和定位方面具有更优异的性能。

### Kalogeiton VS, Weinzaepfel P, Ferrari V, Schmid C. Action Tubelet Detector for Spatio-Temporal Action Localization. In Proc. IEEE Int. Conf. Comput. Vis., 2017, 4415–4423
目前最先进的时空动作定位方法依赖于帧级别的检测，然后跨时间链接或跟踪这些检测。在本文中，我们利用视频的时间连续性，而不是在帧级别操作。我们提出了 ACtion Tubelet 检测器（ACT 检测器），它将帧序列作为输入并输出 tubelet，即带有相关分数的边界框序列。与最先进的物体检测器依赖于锚框一样，我们的 ACT 检测器基于锚长方体。我们以 SSD 框架为基础。为每一帧提取卷积特征，而分数和回归基于这些特征的时间堆叠，从而利用序列中的信息。我们的实验结果表明，利用帧序列比使用单个帧可以显著提高检测性能。我们的 tubelet 检测器的增益可以通过更准确的分数和更精确的定位来解释。我们的 ACT 检测器在 J-HMDB 和 UCF-101 数据集上的表现优于最先进的帧 mAP 和视频 mAP 方法，尤其是在高重叠阈值下。


### Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. In Proc. Int. Conf. Learn. Represent., 2015
在本研究中，我们探究了卷积网络深度对其在大规模图像识别场景下准确率的影响。我们的主要贡献是对使用极小（3x3）卷积滤波器架构的深度递增网络进行了全面的评估，结果表明，通过将深度提升至16-19个权重层，可以显著提升现有配置的性能。这些发现是我们2014年ImageNet挑战赛参赛作品的基础，我们的团队在该比赛中分别获得了定位和分类类别的冠亚军。我们还证明了我们的表征能够很好地推广到其他数据集，并取得了最先进的结果。我们已将两个性能最佳的ConvNet模型公开，以促进深度视觉表征在计算机视觉领域应用的进一步研究。

### Song L, Zhang S, Yu G, Sun H. TACNet: Transition-Aware Context Network for Spatio-Temporal Action Detection. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, 11979–1198
当前最先进的时空动作检测方法取得了令人瞩目的成果，但在时间范围检测方面仍不尽如人意。主要原因是，一些与真实动作类似的模糊状态，即使是训练有素的网络也可能将其视为目标动作。本文将这些模糊样本定义为“过渡状态”，并提出了一个过渡感知上下文网络（TACNet）来区分过渡状态。该网络包含两个主要组件：时间上下文检测器和过渡感知分类器。时间上下文检测器通过构建循环网络，能够以恒定的时间复杂度提取长期上下文信息。过渡感知分类器可以通过同时对动作和过渡状态进行分类来进一步区分过渡状态。因此，该网络可以显著提升时空动作检测的性能。我们在UCF101-24和J-HMDB数据集上对所提出的TACNet进行了广泛的评估。实验结果表明，TACNet 在 JHMDB 上获得了有竞争力的性能，并且在帧 mAP 和视频 mAP 方面明显优于未修剪的 UCF101 24 上最先进的方法。

### Saha S, Singh G, Cuzzolin F. AMTnet: Action-Micro-Tube Regression by End-to-end Trainable Deep Architecture. In Proc. IEEE Int. Conf. Comput. Vis., 2017, 4424–443
目前主流的动作检测方法只能提供次优解，因为它们依赖于帧级检测，然后在后处理步骤中将其组合成“动作管”。本文从根本上摆脱了当前的实践，迈出了设计和实现深度网络架构的第一步，该架构能够对整个视频子集进行分类和回归，从而为动作检测问题提供真正的最优解。在本研究中，我们特别提出了一种新颖的深度网络框架，能够对跨越两个连续视频帧的 3D 区域提案进行回归和分类，其核心是经典区域提案网络 (RPN) 的改进。因此，我们的 3D-RPN 网络能够通过纯粹利用外观来有效地编码动作的时间特征，这与严重依赖昂贵光流图的方法不同。所提出的模型是端到端可训练的，并且可以在单个步骤中针对动作定位和分类进行联合优化。在测试时，网络预测包含两个连续帧的“微管”，并通过一种新算法将这些微管连接成完整的动作管。该算法利用网络学习到的时间编码，将计算时间缩短了 50%。在 J-HMDB-21 和 UCF-101 动作检测数据集上取得的良好结果表明，我们的模型在纯粹依赖外观时确实优于最先进的模型。

### Singh G, Saha S, Cuzzolin F. TraMNet - Transition Matrix Network for Efficient Action Tube Proposals. In Proc. Asian Conf. Comput. Vis., 2018, 1–18
当前最先进的方法通过将二维锚点扩展为帧堆栈上的三维长方体候选框来解决时空动作定位问题，从而生成一组时间上相连的边界框，称为动作微管。然而，这些方法没有考虑到底层锚点候选框假设也应该像演员或摄像机一样在帧与帧之间移动（转换）。假设我们在每一帧中评估n 个二维锚点，那么对于f个连续帧的序列，从每个二维锚点到下一个锚点的可能转换次数约为，即使f的值较小，转换成本也很高。
为了避免这个问题，我们引入了基于转换矩阵的网络(TraMNet)，它依赖于计算锚点提议之间的转换概率，同时最大化它们与跨帧的地面真实边界框的重叠，并通过转换阈值强制稀疏性。由于生成的转换矩阵是稀疏和随机的，因此这将提议假设的搜索空间从减少到阈值矩阵的基数。在训练时，转换特定于特征图的单元位置，因此使用稀疏（高效）转换矩阵来训练网络。在测试时，可以通过降低阈值或向其中添加来自任何单元位置的所有相对转换来获得更密集的转换矩阵，允许网络处理测试数据中可能不存在的转换，从而使检测具有平移不变性。我们证明了我们的网络能够处理诸如 DALY 数据集中可用的稀疏注释，同时允许在单个模型中进行密集（准确）或稀疏（高效）评估。我们在 DALY、UCF101-24 和 Transformed-UCF101-24 数据集上进行了大量的实验，以支持我们的结论。

### Singh G, Choutas V, Saha S, Yu F, Van Gool L. SpatioTemporal Action Detection Under Large Motion. arXiv preprint arXiv:2209.02250, 2022
当前的时空动作管检测方法通常将给定关键帧的边界框候选框扩展为三维时间长方体，并从邻近帧中池化特征。然而，如果由于摄像机运动较大、演员形状变形较大、演员动作较快等原因，导致演员的位置或形状在帧间表现出较大的二维运动和变化，则这种池化操作无法积累有意义的时空特征。本文旨在研究长方体感知特征聚合在大动作下动作检测的性能。此外，我们提出通过跟踪演员并沿相应轨迹执行时间特征聚合来增强大动作下演员的特征表示。我们用动作管/轨迹框在各个固定时间尺度上的交并比 (IoU) 来定义演员的运动。具有较大运动的动作会导致随时间推移的 IoU 较低，而较慢的动作会保持较高的 IoU。我们发现，与长方体感知基线相比，轨迹感知特征聚合在动作检测性能方面始终取得了显著提升，尤其是在大动作下的动作检测中。因此，我们还报告了大规模多项运动数据集的最新成果。

### He J, Deng Z, Ibrahim MS, Mori G. Generic tubelet proposals for action localization. In Proc. IEEE Winter Conf. Appl. Comput. Vis., 2018, 343–351
我们开发了一个用于视频动作定位的新颖框架。我们提出了 Tube Proposal Network (TPN)，它可以在视频中生成通用的、与类别无关的、视频级别的 tubelet 提案。生成的 tubelet 提案可用于各种视频分析任务，包括识别和定位视频中的动作。具体而言，我们将这些通用 tubelet 提案集成到统一的时间深度网络中以进行动作分类。与其他方法相比，我们的通用 tubelet 提案方法准确、通用，并且在 SmoothL1 损失函数下完全可微。我们在标准 UCF-Sports、J-HMDB21 和 UCF-101 数据集上展示了我们算法的性能。我们的与类别无关的 TPN 优于其他 tubelet 生成方法，而我们的统一时间深度网络在这三个数据集上均取得了最先进的定位结果。

### Li Y, Lin W, See J, Xu N, Xu S, Yan K, Yang C. CFAD: Coarse-to-Fine Action Detector for Spatiotemporal Action Localization. In Proc. Eur. Conf. Comput. Vis., 2020, 40–5
当前大多数时空动作定位流程都是将逐帧或逐片段的检测结果连接起来以生成动作提议，但这种做法仅利用了局部信息，而且密集的每帧定位会降低效率。在本文中，我们提出了由粗到细动作检测器 (CFAD)，这是一个新颖的端到端可训练框架，可用于实现高效的时空动作定位。CFAD 引入了一种新范式，首先从视频流中估计粗略的时空动作管，然后根据关键时间戳细化管的位置。该概念由我们框架中的两个关键组件实现：粗略模块和细化模块。粗略模块中长时间信息的参数化建模有助于获得准确的初始管估计，而细化模块则在关键时间戳的指导下选择性地调整管位置。与其他方法相比，所提出的 CFAD 在 UCF101-24、UCFSports 和 JHMDB-21 的动作检测基准上取得了有竞争力的结果，推理速度比最接近的竞争对手快 3.3 倍。

### Su R, Ouyang W, Zhou L, Xu D. Improving action localization by progressive cross-stream cooperation. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, 12016–12025
时空动作定位包含三个层次的任务：空间定位、动作分类和时间分割。在本研究中，我们提出了一种新的渐进式跨流协作 (PCSC) 框架，以迭代方式利用区域提案和来自另一个流（即 RGB/Flow）的特征，迭代改进动作定位结果并为一个流（即 Flow/RGB）生成更好的边界框。具体而言，我们首先通过组合来自两个流的最新区域提案来生成一个更大的区域提案集合，从中我们可以很容易地获得一个更大的带标签训练样本集合，以帮助学习更好的动作检测模型。其次，我们还提出了一种新的消息传递方法，将信息从一个流传递到另一个流，以学习更好的表示，这也能带来更好的动作检测模型。因此，我们的迭代框架在帧级别逐步改进动作定位结果。为了提升视频层面的动作定位结果，我们提出了一种新策略来训练特定类别的动作检测器，以实现更佳的时间分割效果。该策略可以通过使用时间边界附近的训练样本轻松学习。在 UCF-101-24 和 J-HMDB 两个基准数据集上进行的全面实验，证明了我们新提出的时空动作定位方法在现实场景中的有效性。

### Yang X, Yang X, Liu MY, Xiao F, Davis LS, Kautz J. STEP: Spatio-Temporal Progressive Learning for Video Action Detection. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, 264–272
在本文中，我们提出了时空渐进式（STEP）动作检测器——一种用于视频时空动作检测的渐进式学习框架。我们的方法从少量粗尺度的候选框长方体开始，通过几个步骤逐步细化候选框，使其更贴近动作。这样，在后续步骤中，我们能够利用前一步的回归输出，逐步获得高质量的候选框（即，能够贴合动作的移动）。在每一步中，我们都会自适应地扩展候选框，以融入更多相关的时间上下文。与之前一次性完成动作检测的工作相比，我们的渐进式学习框架能够自然地处理动作框内的空间位移，从而为时空建模提供了一种更有效的方法。我们在 UCF101 和 AVA 数据集上对该方法进行了广泛的评估，并展示了卓越的检测结果。值得注意的是，我们分别仅使用了 11 个和 34 个初始候选框，并在两个数据集上分别以 3 个渐进式步骤实现了 75.0% 和 18.6% 的 mAP。

### Li Y, Wang Z, Wang L, Wu G. Actions as Moving Points. In Proc. Eur. Conf. Comput. Vis., 2020, 1–21
现有的动作管检测器通常依赖于启发式锚点的设计和放置，这可能在计算上很昂贵并且对于精确定位而言不是最优的。在本文中，我们提出了一个概念简单、计算高效且更精确的动作管检测框架，称为移动中心检测器（MOC-detector），它将动作实例视为移动点的轨迹。基于运动信息可以简化和辅助动作管检测的见解，我们的 MOC-detector 由三个关键的头部分支组成：（1）中心分支，用于实例中心检测和动作识别，（2）运动分支，用于在相邻帧处进行运动估计以形成移动点的轨迹，（3）框分支，用于通过直接回归每个估计中心的边界框大小来进行空间范围检测。这三个分支协同工作以生成管检测结果，这些结果可以进一步链接起来以产生具有匹配策略的视频级管。我们的 MOC 检测器在 JHMDB 和 UCF101-24 数据集上，无论是帧映射平均数 (frame-mAP) 还是视频映射平均数 (video-mAP)，都超越了现有的最佳方法。当视频 IoU 较高时，性能差距更为明显，这表明我们的 MOC 检测器在更精确的动作检测方面尤其有效。代码位于https://github.com/MCG-NJU/MOC-Detector。

### Liu Y, Yang F, Ginhac D. Accumulated micro-motion representations for lightweight online action detection in real-time. Journal of Visual Communication and Image Representation, 2023: 103879
在过去的十年中，视觉传感器和视频内容的爆炸式增长推动了对空间和时间中人体动作检测自动化的众多应用需求。除了可靠的精度之外，广泛的现实世界场景还要求在有限的计算预算下对动作进行连续和即时的处理。然而，现有的研究通常依赖于 3D 卷积和细粒度光流等繁重的操作，因此在实际部署中受到阻碍。严格以更好地结合在线检测的检测精度、速度和复杂度为目标，我们定制了一个基于 2D-CNN 的经济高效的小管检测框架，称为累积微运动动作检测器（AMMA）。它稀疏地提取和融合跨越较长时间窗口的动作的视觉动态线索。为了摆脱对昂贵的光流估计的依赖，AMMA 有效地将动作的短期动态编码为来自 RGB 帧的即时累积微运动。在 AMMA 的运动感知二维主干模型之上，我们采用无锚点检测器，将动作实例协同建模为时间跨度内的移动点。我们提出的动作检测器实现了与当前最佳模型相当的出色准确率，同时显著降低了模型大小、计算成本和处理时间（分别为 600 万个参数、1 GMAC 和 100 FPS），使其在严格的速度和计算约束下更具吸引力。代码可在https://github.com/alphadadajuju/AMMA获取。

### Duarte K, Rawat Y, Shah M. Videocapsulenet: A simplified network for action detection. Proc. Adv. Neural Inf. Process. Syst., 2018, 31
深度卷积神经网络 (DCNN) 的最新进展已在视频人体动作分类方面取得了优异的成果，然而，动作检测仍然是一个极具挑战性的问题。目前的动作检测方法遵循复杂的流程，涉及多个任务，例如管道提案、光流和管道分类。在本研究中，我们基于最新开发的胶囊网络，提出了一种更优雅的动作检测解决方案。我们提出了一个用于视频的 3D 胶囊网络，称为 VideoCapsuleNet：一个统一的动作检测网络，可以同时执行像素级动作分割和动作分类。该网络是胶囊网络从 2D 到 3D 的泛化，以视频帧序列作为输入。3D 泛化显著增加了网络中胶囊的数量，使得胶囊路由的计算成本高昂。我们在卷积胶囊层引入胶囊池化来解决这个问题，并使投票算法更易于处理。网络中的协议路由机制固有地对动作表示进行建模，并且各种动作特征都被预测的胶囊捕获。这启发我们利用胶囊进行动作定位，并使用网络预测的类别特定胶囊来确定动作的像素级定位。通过与卷积胶囊层进行参数化的跳跃连接，进一步提升了定位效果，并且网络采用分类和定位损失进行端到端训练。所提出的网络在多个动作检测数据集（包括 UCF-Sports、J-HMDB 和 UCF-101（24 个类别））上实现了最佳性能，在 v-mAP 得分方面，UCF-101 提高了约 20%，J-HMDB 提高了约 15%。

### Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S. End-to-end object detection with transformers. In Proc. Eur. Conf. Comput. Vis., 2020, 213–22
我们提出了一种新方法，将目标检测视为一个直接的集合预测问题。我们的方法简化了检测流程，有效地消除了许多手工设计的组件，例如非最大抑制程序或锚点生成，这些组件明确地编码了我们对该任务的先验知识。这个新框架名为 DEtection TRansformer 或 DETR，其主要组成部分包括基于集合的全局损失函数（通过二分匹配强制进行唯一预测）以及一个 Transformer 编码器-解码器架构。给定一小组固定的已学习目标查询，DETR 推理目标与全局图像上下文之间的关系，从而直接并行输出最终的预测集合。与许多其他现代检测器不同，这个新模型概念简单，不需要专门的库。在具有挑战性的 COCO 目标检测数据集上，DETR 的准确率和运行时性能与成熟且高度优化的 Faster R-CNN 基线相当。此外，DETR 可以轻松推广，以统一的方式生成全景分割。我们证明它的性能显著优于其他竞争基线。训练代码和预训练模型可在https://github.com/facebookresearch/detr上找到。

### Zhao J, Zhang Y, Li X, Chen H, Shuai B, Xu M, Liu C, Kundu K, Xiong Y, Modolo D, et al.. TubeR: Tubelet transformer for video action detection. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, 13598–1360
我们提出了 TubeR：一种用于时空视频动作检测的简单解决方案。与依赖于离线参与者检测器或手工设计的行为者位置假设（如提案或锚点）的现有方法不同，我们提出通过同时从单一表示执行动作定位和识别来直接检测视频中的动作管。TubeR 学习一组管查询并利用管注意模块来建模视频片段的动态时空特性，与在时空空间中使用参与者位置假设相比，这有效地增强了模型容量。对于包含过渡状态或场景变化的视频，我们提出了一个上下文感知分类头，利用短期和长期上下文来加强动作分类，以及一个动作切换回归头来检测精确的时间动作程度。TubeR 直接生成长度可变的动作管，即使对于长视频片段也能保持良好的效果。 TubeR 在常用动作检测数据集 AVA、UCF101-24 和 JHMDB51-21 上的表现超越了之前的最优结果。代码将在 GluonCV (https://cv.gluon.ai/) 上发布。

### Li D, Yao T, Qiu Z, Li H, Mei T. Long Short-Term Relation Networks for Video Action Detection. In Proc. ACM Multimedia Conf., 2019, 629–63
众所周知，对人与物体或物体与物体关系进行建模将有助于检测任务。然而，这个问题并不简单，特别是在探索人类演员、物体和场景（统称为人类-语境）之间的相互作用以增强视频动作检测器时。困难源于这样一个方面，即视频中可靠的关系不仅应取决于当前剪辑中的短期人与语境关系，还应取决于视频长距离跨度上提炼的时间动态。这促使我们捕捉视频中的短期和长期关系。在本文中，我们提出了一种新的长短期关系网络，称为 LSTR，它新颖地聚合和传播关系以增强视频动作检测的特征。从技术上讲，区域提议网络 (RPN) 被重塑为首先在每个视频片段中生成 3D 边界框，即小管。然后，LSTR 通过时空注意力机制对每个片段中的短期人机交互进行建模，并通过图卷积网络 (GCN) 以级联方式推理跨视频片段的长期时间动态。在四个基准数据集上进行了广泛的实验，并报告了与最先进方法相比更优的结果。

### Zhang Y, Tokmakov P, Hebert M, Schmid C. A structured model for action detection. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, 9975–9984
计算机视觉领域基于学习的方法的主要范式是在大型数据集上训练通用模型，例如用于图像识别的 ResNet 或用于视频理解的 I3D，并让它们找到针对当前问题的最佳表示。虽然这种方法显然很有吸引力，但并非适用于所有场景。我们认为动作检测就是这样一个具有挑战性的问题——需要训练的模型规模庞大，而获取标记数据的成本高昂。为了突破这一限制，我们建议将领域知识融入模型结构，从而简化优化过程。具体而言，我们在标准 I3D 网络中添加了一个跟踪模块，以聚合长期运动模式，并使用图卷积网络推理参与者与物体之间的交互。在具有挑战性的 AVA 数据集上进行评估，所提出的方法比 I3D 基线提高了 5.5% 的 mAP，比最先进的方法提高了 4.8% 的 mAP。

### End-to-end spatio-temporal action localisation with video transformers
性能最高的时空动作定位模型使用外部人员提案和复杂的外部存储库。我们提出了一个完全端到端的基于 Transformer 的模型，该模型直接摄取输入视频并输出小管——一系列边界框和每帧的动作类别。我们灵活的模型可以使用对单个帧的稀疏边界框监督或完整的小管注释进行训练。在这两种情况下，它都将连贯的小管预测为输出。此外，我们的端到端模型不需要以提案的形式进行额外的预处理，也不需要以非最大抑制的形式进行后处理。我们进行了广泛的消融实验，并在五个不同的时空动作定位基准上显著提升了当前最佳水平，这些基准既包含稀疏关键帧，也包含完整的小管注释。

### Com-STAL: Compositional spatio-temporal action localization
时空动作定位旨在定位行动者的空间和时间位置并对其动作进行分类。然而，先前的研究忽略了现实场景中人类动作经常与新物体交互的事实，从而忽略了动作-物体的各种组合，并极大地限制了所开发模型的泛化能力。本文通过研究动作-物体组合的多模态视觉信息来研究它们。为此，我们提出了一种新颖的组合时空动作定位（Com-STAL）任务，该任务在训练集和测试集中以不重叠的动作-物体组合为特征。基于此，我们构建了一个组合动作定位数据集（Com-AD）。在此基础上，我们提出了一个简单而有效的框架——以实例为中心的交互网络（ICIN），以减少视觉模态中的无效归纳偏差，并通过利用额外的模态信息来缓解组合分布偏差问题。在 Com-AD 上的大量实验结果证明了 ICIN 卓越的动作定位性能。

### Spatio-temporal human action localization in indoor surveillances
时空动作定位是视频理解领域中一项至关重要且颇具挑战性的任务。现有的时空动作检测基准受到诸如注释不完整、高级非通用动作以及不常见场景等因素的限制。为了解决这些限制并促进其在实际安全应用中的研究，我们引入了一个全新的以人为中心的数据集，用于室内监控环境中原子动作的时空定位，称为 HIA（以人为中心的室内动作）。HIA 数据集通过选择 30 个原子动作类别、编译 100 个监控视频，并使用 370,937 个边界框注释了 219,225 帧。HIA 的主要特点包括：（1）对原子动作提供准确的时空注释，（2）在帧级别提供以人为中心的注释，（3）在不连续的轨迹上对人员进行时间链接，以及（4）利用室内监控视频。我们的 HIA 具有在室内监控场景中的真实设置和全面的注释，对时空动作定位领域提出了有价值且新颖的挑战。为了建立基准，我们评估了各种方法并对 HIA 数据集进行了深入分析。HIA 数据集即将发布，我们预计它将成为研究界的标准和实用基准。

### You watch once more: a more effective CNN architecture for video spatio-temporal action localization
时空动作定位（STAL）任务需要检测场景中个体的动作和位置。许多研究无法很好地建模时空信息，并且通常忽略了推理速度和实际应用。针对上述问题，我们提出了一种名为“You Watch Once More”（YWOM）的新型端到端时空动作定位网络。该网络采用两个主干网络有效地提取时空信息。本文提出了三项措施来提高定位和识别的准确性，同时保证推理速度。首先，提出了一种基于频域注意（FCA）的新型特征融合机制，可以有效地融合不同主干网络提取的特征。此外，提出了一种新的损失函数来加速边界框的回归和收敛。具体而言，我们使用SIOU回归损失函数代替平滑的L1损失函数来帮助模型稳定收敛。此外，我们还设计了一种横向连接机制，以便在网络结构中应用更多的主干网络。实验结果表明，YWOM 能够达到在线推理速度，在时空动作定位任务中表现出色。在 UCF101-24 数据集上，YWOM 优于包括 YOWO 在内的其他相关工作，帧 mAP 和视频 mAP（0.2）分别提升了 4.23% 和 1.63%。

### Leveraging Multimodal Knowledge for Spatio-Temporal Action Localization
在混乱场景中定位人员并识别其行为是视频理解中一项更具挑战性的任务。与人类日常行为不同，混乱事件中的行为在执行方式及其对周围个体的影响方面存在显著差异，从而增加了复杂性。传统的时空动作定位方法仅依赖于单一视觉模态，在复杂场景中难以奏效。本文利用大型语言模型 (LLM) 和视觉语言 (VL) 基础模型，从多模态视角探索 STAL。我们分析了视觉 STAL 固有的特征聚合阶段，并介绍了一种专为 VL 基础模型定制的知识聚合方法，称为多模态基础知识集成 (MFKI)。MFKI 包含一个通用解码器，用于将 VL 基础模型中的知识与动作特征关联起来，以及一个用于视觉 STAL 中关系推理的专用解码器。MFKI 将通用视觉表征与特定视频特征相结合，以满足复杂 STAL 任务的需求。此外，我们利用 LLM（即 GPT）和提示来丰富标签增强，从而促进对复杂动作更全面的语言理解。在 Chaotic World 数据集上的实验证明了本文提出方法的有效性。代码可从 https://github.com/CKK-coder/Chaotic_World/tree/master 获取。

### Spatio-Temporal Activity Detection via Joint Optimization of Spatial and Temporal Localization
本文探讨时空活动检测问题，该问题需要对视频中人类活动在空间和时间上的进行分类和定位。为此，我们提出了一个新颖的单阶段、端到端可训练深度学习框架，可以联合优化活动的空间和时间定位。该框架利用共享的时空特征图，在单个网络中执行参与者检测、活动管线构建以及活动的时间定位。在极具挑战性的 UCF101-24 基准测试中，该框架的表现优于当前最先进的时空活动检测方法。仅使用 RGB 输入，该框架实现了 60.1% 的视频 mAP，而当同时使用 RGB 和 FLOW 输入时，该标准进一步提升至 61.3%。此外，它还获得了极具竞争力的 74.9% 的帧 mAP。

### Local and Global Context Reasoning for Spatio-Temporal Action Localization
从视频中定位人物并识别其动作是视频理解的一项重要任务。近期，推理行为者之间以及行为者与环境之间的关系取得了进展。然而，对图像进行全局推理并非总是有效的方式，在某些情况下，局部搜索相关线索更为合适。在本文中，我们更进一步，对行为者与其相关周围环境之间的关系进行建模。我们开发了一个流程，该流程通过观察整幅图像来收集全局环境信息，并通过观察行为者周围环境来收集本地环境信息。这通过实施一个专注于局部推理环境信息的近行为者关系网络 (NARN) 来实现。我们的 NARN 的两个关键组件能够有效地积累局部环境信息：姿态编码（将人体姿态信息编码为附加特征）和空间注意力（将相对环境信息与其他信息区分开来）。我们的流程积累全局和局部关系信息，并将它们汇总起来用于最终的动作分类。在 JHMDB21 和 AVA 数据集上的实验结果表明，我们提出的流程优于仅推理全局上下文的基准方法。学习到的注意力图的可视化表明，我们的流程能够聚焦于包含每个动作相关上下文信息的空间区域。

### KORSAL: Key-Point Based Online Real-Time Spatio-Temporal Action Localization
视频中的实时在线动作定位是一项关键且艰巨的挑战。实现精确的动作定位需要整合时间和空间信息。然而，现有方法依赖于计算密集型的 3D 卷积神经网络 (CNN) 架构或带有光流的冗余双流架构，使其不适用于实时在线应用。为了解决这个问题，我们提出了一种新颖的方法，利用快速高效的基于关键点的边界框预测进行空间动作定位。此外，我们引入了一种管连接算法，即使在存在遮挡的情况下也能确保动作管的时间连续性。通过将时间和空间信息组合成单个网络的级联输入，我们无需双流架构，从而使网络能够有效地从两种类型的信息中学习。我们没有使用计算量巨大的光流，而是使用结构相似性索引图来高效地提取时间信息。尽管我们的方法非常简单，但我们轻量级的端到端架构在极具挑战性的 UCF101-24 数据集上实现了 74.7% 的当前最佳帧平均精度 (mAP)，相比以往的在线方法显著提升了 6.4%。此外，与在线和离线方法相比，我们实现了当前最佳的视频 mAP 结果。此外，我们的模型实现了 41.8 FPS（每秒帧数）的帧率，比当代的实时方法提升了 10.7%。

### ContextLoc++: A unified context model for temporal action localization
有效解决时间动作定位 (TAL) 问题需要一种视觉表征，该表征能够同时追求两个相互混淆的目标：时间定位的细粒度判别和动作分类的充分视觉不变性。我们通过在流行的两阶段时间定位框架中丰富局部、全局和多尺度上下文来应对这一挑战。我们提出的模型 ContextLoc++ 可分为三个子网络：L-Net、G-Net 和 M-Net。L-Net 通过对片段级特征进行细粒度建模来丰富局部上下文，该过程被表述为查询和检索过程。此外，空间和时间片段级特征（用作键和值）通过时间门控进行融合。G-Net 通过对视频级表征进行更高级的建模来丰富全局上下文。此外，我们还引入了一个新颖的上下文自适应模块，以使全局上下文适应不同的提案。 M-Net 进一步将局部和全局上下文与多尺度候选框特征融合。具体而言，来自多尺度视频片段的候选框级特征可以聚焦不同的动作特征。帧数较少的短期片段关注动作细节，而帧数较多的长期片段则关注动作变化。在 THUMOS14 和 ActivityNet v1.3 数据集上进行的实验验证了我们的方法相对于现有最佳 TAL 算法的有效性。

### Open-vocabulary spatio-temporal action detection
时空动作检测 (STAD) 是一项重要的细粒度视频理解任务。当前的方法需要预先对所有动作类别进行标注框和标签监督。然而，在实际应用中，由于动作类别空间庞大且难以枚举，因此很可能会遇到训练中未曾见过的新动作类别。此外，对于传统方法而言，新类别的数据标注和模型训练成本极高，因为我们需要进行详细的标注框并从头开始重新训练整个网络。本文提出了一种新的挑战性设置，即执行开放词汇时空动作检测 (STAD)，以更好地模拟开放世界中的动作检测情况。开放词汇时空动作检测 (OV-STAD) 需要在有限的基类集合上进行模型训练，并使用标注框和标签监督，预计该方法将在新的动作类别上取得良好的泛化性能。对于 OV-STAD，我们基于现有的 STAD 数据集构建了两个基准测试，并提出了一种基于预训练视频语言模型 (VLM) 的简单有效方法。为了使整体 VLM 更好地适应细粒度动作检测任务，我们针对局部视频区域-文本对对其进行了细致的微调。这种定制化的微调使 VLM 能够更好地理解运动，从而有助于更准确地对齐视频区域和文本。在对齐之前，我们采用了局部区域特征和全局视频特征融合，通过提供全局上下文信息来进一步提升动作检测性能。我们的方法在新的类别上取得了令人欣喜的表现。

### Yowov2: A stronger yet efficient multi-level detection framework for real-time spatio-temporal action detection
设计一个实时的时空动作检测框架仍然是一个挑战。在本文中，我们提出了一个新颖的实时动作检测框架YOWOv2。在这个新框架中，YOWOv2同时利用3D骨干网络和2D骨干网络进行精确的动作检测。我们设计了一个多级检测流程来检测不同尺度的动作实例。为了实现这一目标，我们精心构建了一个简单高效的2D骨干网络，并使用特征金字塔网络来提取不同级别的分类特征和回归特征。对于3D骨干网络，我们采用现有的高效3D CNN以节省开发时间。通过组合不同尺寸的3D骨干网络和2D骨干网络，我们设计了一个YOWOv2系列，包括YOWOv2-Tiny、YOWOv2-Medium和YOWOv2-Large。我们还引入了流行的动态标签分配策略和无锚机制，使YOWOv2与先进的模型架构设计保持一致。经过我们的改进，YOWOv2 的性能显著优于 YOWO，并且仍然保持了实时检测的性能。YOWOv2 在 UCF101-24 数据集上，帧间平均准确率 (mAP) 达到 87.0%，视频 mAP 达到 52.8%，帧率超过 20 FPS。在 AVA 数据集上，YOWOv2 帧间平均准确率 (mAP) 达到 21.7%，帧率超过 20 FPS。我们的代码可在https://github.com/yjh0410/YOWOv2获取。

### Cross time-frequency transformer for temporal action localization
目前，大多数时间动作定位 (TAL) 方法主要关注时间域信息，而忽略了其他域信息的优势。如何有效利用不同域的信息及其相互作用一直是 TAL 领域中一个极具吸引力但又极具挑战性的课题。本文提出了一种用于 TAL 的新型跨时频 Transformer 模型 (TFFormer)。设计了一种双分支网络架构，用于捕捉多个尺度的时间和频率特征，在时间分支中使用多尺度 Transformer，在频率分支中使用 DB1 离散小波变换 (DWT)。为了融合不同域的特征，我们提出了一种跨时频注意机制，该机制包含时间通路和频率通路，增强了时间和频率特征之间的相互作用。此外，我们还设计了一种门控机制来聚合不同尺度的特征，以表征不同尺度特征各自的贡献。我们还设计了一种新的回归损失函数来定位时间边界。在四个具有挑战性的基准数据集上进行了大量的实验，其中包括两个第三人称数据集和两个第一人称数据集。所提出的方法在这些数据集上取得了令人印象深刻的结果。具体而言，TFFormer 在 Ego4D 上的平均 mAP 为 23.2%，在 EPIC-Kitchens 100 上的平均 mAP 为 25.6%，大幅超越了之前的最优结果。它在 ActivityNet v1.3 和 THUMOS14 上也取得了具有竞争力的结果，平均 mAP 分别为 36.2% 和 67.8%。我们还进行了大量的消融研究，以验证所提出方法中每个组件的有效性。

### Interaction-aware prompting for zero-shot spatio-temporal action detection
时空动作检测的目标是确定视频中每个人动作发生的时间和地点，并对其进行相应的动作类别分类。现有方法大多采用全监督学习，需要大量的训练数据，难以实现零样本学习。本文提出利用预训练的视觉语言模型提取具有代表性的图像和文本特征，并通过不同的交互模块建模这些特征之间的关系以获取交互特征。此外，我们利用此特征对每个标签进行提示，以获得更合适的文本特征。最后，我们计算每个标签的交互特征与文本特征的相似度，以确定动作类别。我们在 J-HMDB 和 UCF101-24 数据集上的实验表明，所提出的交互模块和提示机制使视觉语言特征更加匹配，从而在零样本时空动作检测中取得了优异的准确率。代码将在 https://github.com/webber2933/iCLIP 上发布。

### Online spatio-temporal action detection with adaptive sampling and hierarchical modulation
在线时空动作检测 (OSTAD) 是视频理解中的一项关键任务，负责以在线方式识别和分类视频流中的动作实例。本文提出了一种新颖的方法，该方法采用自适应采样和分层调制来增强 OSTAD 性能。传统方法通常受固定采样率的限制，在动作速度较慢的场景中可能导致冗余，并在快速移动的序列中忽略重要细节。我们创新的动态采样策略基于速度估计，可根据速度注意力和视觉差异特征自适应地调整采样间隔，从而优化每个采样视频片段的信息内容。此外，我们的方法还结合了分层调制机制，协同高级语义和低级空间信息，显著提高了动作定位和分类的准确性。在这些改进的基础上，基于分层调制的自适应采样网络在 JHMDB21 和 UCF24 等基准数据集上取得了显著的提升，证明了我们的方法在在线环境中处理多样化和动态动作序列的有效性。

### MLSTIF: multi-level spatio-temporal and human-object interaction feature fusion network for spatio-temporal action detection
人机交互 (HOI) 信息为时空动作检测 (STAD) 提供线索或约束。现有的包含 HOI 关系的 STAD 模型利用重量级的 3D 主干网络提取时间信息，并且需要在 HOI 特征生成阶段匹配大量区域提案，这会导致显著的计算开销。为了突破这些限制，我们提出了一个多级时空与交互特征融合 (MLSTIF) 网络，其 2D 和 3D 主干网络分别通过 You Only Look Once (YOLO)v7 和 3D-EfficientNetv2 实现，用于提取多级空间和时空特征。为了充分利用视频中的时空信息，设计了空间与时空特征融合（SSTFF）模块，将多级时空特征获取的运动信息融合到相应层级的空间特征中；同时为了更有效地从视频中提取与运动目标相关的交互内容，设计了解耦可变形HOI变换器（Decoupled Deformable HOITR）模块，产生高阶分类与回归交互特征，提升运动目标时空位置和动作类别的准确率。与实验中所有同类方法相比，MLSTIF在UCF101-24数据集上的准确率提升了1.4%~11.3%，在J-HMDB数据集上的准确率提升了0.9%~11.4%。在同类方法中，部分模型的计算成本至少是MLSTIF的3倍。与YOWO系列、YWOM等计算成本相当的模型相比，MLSTIF在AVA v2.2数据集上的准确率提高了1.4-7.2%。

### Hierarchical chat-based strategies with MLLMs for Spatio-temporal action detection
足球比赛中的时空动作检测 (STAD) 因其涉及多名参与者的细微且快节奏的动作而极具挑战性。多模态大型语言模型 (MLLM) 通常无法通过标准提示捕捉这些细微差别，导致结果缺乏改进视觉特征所需的详细描述。为了解决这个问题，我们提出了一种名为“基于分层聊天策略 (HCBS)”的提示策略。具体而言，该策略使 MLLM 能够形成思路链 (CoT)，逐步生成包含更详细信息的内容。我们在三个数据集上进行了广泛的实验：来自 Multisports 的 126 个视频、来自 J-HMDB 的 43 个视频以及来自 UCF101-24 的 147 个视频，所有视频均侧重于足球部分。与基准任务相比，我们的方法在这三个数据集上的性能分别提升了 30.3%、26.1% 和 25.5%。通过层次验证实验，我们证明了 HCBS 能够有效地指导 MLLM 生成层次化描述。此外，我们使用 HCBS 指导 MLLM 进行内容生成，创建了一个包含 120,511 个帧描述的帧级描述数据集，涵盖三个数据集。我们的代码和数据集可在以下链接获取：

### Action tube generation by person query matching for spatio-temporal action detection
本文提出了一种时空动作检测 (STAD) 方法，该方法直接从原始视频生成动作管，而不依赖于基于 IoU 的链接和片段分割等后处理步骤。我们的方法将基于查询的检测 (DETR) 应用于每一帧，并匹配 DETR 查询以在各个帧之间链接同一个人。我们引入了查询匹配模块 (QMM)，它使用度量学习使针对同一个人的查询在各个帧之间比针对不同人的查询更紧密地联系在一起。使用从 QMM 匹配中获得的查询序列来预测动作类别，允许从长度超过单个片段的视频中获得可变长度的输入。在 JHMDB、UCF101-24 和 AVA 数据集上的实验结果表明，我们的方法在人物位置发生较大变化时表现良好，同时提供卓越的计算效率和更低的资源需求。

### ACSF-ED: Adaptive Cross-Scale Fusion Encoder-Decoder for Spatio-Temporal Action Detection.
当前的时空行为检测方法在提取和理解时空信息方面能力不足。本文提出一种端到端的自适应跨尺度融合编解码器（ACSF-ED）网络，用于高效地预测动作和定位目标。在自适应跨尺度融合时空编码器（ACSF ST-Encoder）中，设计了渐近跨尺度特征融合模块（ACCFM）来解决高级语义信息传播带来的信息退化问题，从而提取高质量的多尺度特征，为后续的时空信息建模提供更优的特征。在共享头解码器结构中，构建了一个共享的分类和回归检测头。设计了一种由一对一、一对多和对比去噪损失组成的多约束损失函数，解决了传统方法预测结果时约束力不足的问题。该损失函数提高了模型分类预测的准确率，并改善了回归位置预测与地面真实目标的接近度。在热门数据集 UCF101-24 和 JHMDB-21 上对所提方法模型进行了评估。实验结果表明，所提方法在 Frame-mAP 指标上达到了 81.52% 的准确率，超越了现有方法。

### YOWOv3: A Lightweight Spatio-Temporal Joint Network for Video Action Detection
时空动作检测网络需要同时提取和融合空间和时间特征，这往往导致现有模型过于臃肿，难以实时运行和部署在边缘设备上。本文提出了一种高效实时的时空动作检测模型YOWOv3。该模型采用高效的三维和二维骨干网络，分别从序列信息中提取空间特征和时空特征。此外，通过深度融合卷积和自注意力机制，设计了一个轻量级的时空特征融合模块，进一步增强了时空特征的提取。我们称该模块为CFACM（通道融合与注意力卷积混合）模块。我们的方法不仅在轻量化方面超越了最新的高效时空动作检测模型，模型尺寸相比后者减小了24%，而且在UCF101-24数据集上mAP准确率提升了1.35%，同时保持了优异的速度性能，实现了准确率与速度的平衡。另外，现有模型往往使用3D卷积来提取时间信息，这在某些设备（如苹果M系列处理器）上可能会受到限制。为了缓解时空动作检测模型在边缘部署时可能不支持3D卷积算子的问题，我们采用了一个仅包含2D卷积的时空移位模块，使模型能够获取时间信息，并将获取的时间特征注入到多级时空特征提取模型中，不仅将模型从3D卷积操作的束缚中解放出来，也提升了模型在准确率与速度之间的平衡。这使得仅使用 2D 卷积的轻量级网络实现了最先进的性能。

### Spatio-temporal action detector with cross-stream knowledge transfer
依靠人力通过监控系统进行不间断监控效率低下且容易被疏忽，这使得智能监控具有巨大的应用前景和价值。为了实现该技术，通常使用时空动作检测器，这是一种能够同时在时间和空间维度上定位动作实例的神经网络。为了解决三维骨干网络通常伴随的高计算需求问题，我们提出了一种基于二维骨干网络的动态跨流移动中心检测器 (DCMOC)，它是 MOC 检测器的改进版本。在 DCMOC 检测器中，运动分支和边界框分支分别对应动作实例的运动特征和外观特征。通过设计的跨流知识迁移模块，在两个分支之间迁移特征知识以增强特征表示。为了与现有的时空动作检测器进行比较，我们在 JHMDB 和 UCF101-24 两个数据集上进行了实验。实验表明，DCMOC 检测器实现了具有竞争力的性能，其计算要求明显低于现有的 3D 架构模型，并且在高阈值下在视频地图指标上实现了最佳性能。

### patio-Temporal Action Detector with Self-Attention
在时空动作检测领域，一些当前的研究尝试利用基于无锚点的单阶段目标检测器来解决动作检测问题。尽管效率有所提高，但仍有望获得更大的性能提升。为此，提出了一种自注意移动中心检测器 (SAMOC)，它具有两个吸引人的特点：1）为了有效地捕捉运动线索，我们探索了时空自注意模块，通过聚合与运动相关的全局上下文来增强特征表示；2）链接分支用于建模帧级对象依赖性，从而提高正确动作的置信度得分。在两个基准数据集上的实验表明，基于这两个特点的 SAMOC 达到了最佳水平，并且能够实时运行。
